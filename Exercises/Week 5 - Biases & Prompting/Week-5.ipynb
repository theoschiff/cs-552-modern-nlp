{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qU4Y9G1pZFW"
      },
      "source": [
        "# ðŸ“š  Exercise Session - Week 5\n",
        "\n",
        "Welcome to Week 5 exercise session of CS552-Modern NLP!\n",
        "\n",
        "We will continue playing with `DistilBert` this week, and learn about the dataset biases and prompting.\n",
        "\n",
        "[Part 1: Biases](#bias0)\n",
        "- [1.1 Hypothesis only NLI](#bias1)\n",
        "\n",
        "[Part 2: Prompting](#prompt0)\n",
        "- [2.1 Zero-shot Prompting](#promp1)\n",
        "- [2.2 Few-shot Prompting](#promt2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97PgcwMKpZY4"
      },
      "source": [
        "<a name=\"bias0\"></a>\n",
        "## 1. Biases\n",
        "\n",
        "Recall our knowledge about the NLI tasks, the model would be given a pair of sentence: `(premise, hypothesis)`, and needs to judge the relationship between them. Specifically, given the *premise*, if the *hypothesis* is **true (entailment)**, **false (condradiction)**, or **neither (neutral)**. Idealy, The label of the hypothesis should be entirely based upon the given premise. However, *if the model is able to correctly guess the label without seeing the premise, it is likely detecting biased statisitcal patterns that are undesirable*, such as tendency to use certain words among different classes (ex: using negation words such as 'not' for the contradiction label).\n",
        "\n",
        "Inspired by the paper [Hypothesis Only Baselines in Natural Language Inference](https://aclanthology.org/S18-2023.pdf), the first part of this lab will investigate a classifier's internal bias when performing the NLI task by testing its hypothesis-only performance.\n",
        "\n",
        "**`Note`** In this dataset the labels are as follows: `0-Entailment`, `1- Neutral`, and `2- Contradict`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lxexYkR3pZsc"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install jsonpickle\n",
        "!pip install datasets\n",
        "!pip install accelerate -U"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yb7ZmtDHyKAb"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import jsonpickle\n",
        "import os\n",
        "import sys\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import trange, tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import List, Dict, Optional\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import RandomSampler, DataLoader, SequentialSampler\n",
        "\n",
        "import datasets\n",
        "from datasets import load_dataset\n",
        "\n",
        "from transformers import RobertaForMaskedLM,RobertaTokenizer, RobertaForSequenceClassification, DistilBertTokenizer, DistilBertForSequenceClassification\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "\n",
        "from sklearn.metrics import accuracy_score, f1_score\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3I7D8ul5ikM"
      },
      "source": [
        "<a name=\"bias1\"></a>\n",
        "## 1.1 Train: Hypothesis only NLI\n",
        "\n",
        "Let's firstly train a `distilbert` model on the SNLI dataset, but only access the hypothesis. We reuse the functions from the Exercise4."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_pretrained(model_name, num_labels=2):\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "  model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
        "\n",
        "  if torch.cuda.is_available():\n",
        "    model = model.to('cuda:0')\n",
        "  return tokenizer, model"
      ],
      "metadata": {
        "id": "mgAGjzpYn-Ws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = load_dataset(\"snli\", split='train')\n",
        "test_dataset = load_dataset(\"snli\", split='test')\n",
        "train_dataset = train_dataset.filter(lambda example: example[\"label\"]!=-1)\n",
        "test_dataset = test_dataset.filter(lambda example: example[\"label\"]!=-1)\n",
        "print('#Training samples: ', len(train_dataset))\n",
        "print('#Test samples: ', len(test_dataset))\n",
        "\n",
        "tokenizer, model = load_pretrained('distilbert-base-uncased', num_labels=3)"
      ],
      "metadata": {
        "id": "LnsR5KXGntJd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
      ],
      "metadata": {
        "id": "sbNnn1CMnzhC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model_nli(model, tokenizer, test_loader):\n",
        "  all_labels = None\n",
        "  all_preds = None\n",
        "\n",
        "  for b in tqdm(test_loader):\n",
        "    premise = b['premise']\n",
        "    hypothesis = b['hypothesis']\n",
        "    label = b['label']\n",
        "\n",
        "    # TODO: tokenize the text\n",
        "    inputs = tokenizer(premise, hypothesis, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "    if torch.cuda.is_available():\n",
        "      inputs = inputs.to('cuda:0')\n",
        "      label = label.to('cuda:0')\n",
        "\n",
        "    # TODO: run the model to make the prediction\n",
        "    with torch.no_grad():\n",
        "        pred = model(**inputs).logits.argmax(dim=-1)\n",
        "\n",
        "    if all_labels is None:\n",
        "      all_labels = label.cpu()\n",
        "      all_preds = pred.cpu()\n",
        "    else:\n",
        "      all_labels = torch.concat([all_labels, label.cpu()])\n",
        "      all_preds = torch.concat([all_preds, pred.cpu()])\n",
        "\n",
        "  assert len(all_preds)==len(all_labels), 'Test Failed. Check your code!'\n",
        "  # compute f1 score between model predictions and ground-truth labels (you can use sklearn.metrics)\n",
        "  f1 = f1_score(all_labels, all_preds, average='macro')\n",
        "\n",
        "  # compute accuracy score between model predictions and ground-truth labels (you can use sklearn.metrics)\n",
        "  acc = accuracy_score(all_labels, all_preds)\n",
        "\n",
        "  # compute the accuracy on Entailment(label==0) samples\n",
        "  entailment_acc = accuracy_score(all_labels[all_labels==0], all_preds[all_labels==0])\n",
        "\n",
        "  # compute the accuracy on Neutral(label==1) samples\n",
        "  neutral_acc = accuracy_score(all_labels[all_labels==1], all_preds[all_labels==1])\n",
        "\n",
        "  # compute the accuracy on Contradict(label==1) samples\n",
        "  contradict_acc = accuracy_score(all_labels[all_labels==2], all_preds[all_labels==2])\n",
        "\n",
        "  print('Accuracy: ', acc*100, '%')\n",
        "  print(' -- Entailment Accuracy: ', entailment_acc*100, '%')\n",
        "  print(' -- Neutral Accuracy: ', neutral_acc*100, '%')\n",
        "  print(' -- Contradict Accuracy: ', contradict_acc*100, '%')\n",
        "  print('F1 score: ', f1)\n",
        "\n",
        "  return all_preds, all_labels, acc, f1"
      ],
      "metadata": {
        "id": "X7pBz6KopBNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ETS: <1min on colab T4 gpu\n",
        "all_preds, all_labels, acc, f1 = evaluate_model_nli(model, tokenizer, test_loader)"
      ],
      "metadata": {
        "id": "xbMBYWF0IHPQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`TODO-1`: Implement the `tokenize_function` to tokenize only the `hypothesis` in each input `examples`."
      ],
      "metadata": {
        "id": "U9ezY5R5mCMs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Define a function to tokenize the text\n",
        "def tokenize_function(examples, hyp_only=True, max_length=512, device='cuda:0'):\n",
        "  '''\n",
        "  INPUT:\n",
        "    examples: input samples in the dataset\n",
        "    hyp_only: if True, only tokenize the \"hypothesis\"; tokenize both \"premise\" and \"hypothesis\" if False\n",
        "    max_length: maximal number of tokens\n",
        "    device: cuda or cpu\n",
        "  OUTPUT:\n",
        "    tokenized: tokenized sample, truncation=True, padding=True\n",
        "  '''\n",
        "  if hyp_only:\n",
        "    tokenized = ...\n",
        "  else:\n",
        "    tokenized = ...\n",
        "  return tokenized\n",
        "\n",
        "# Tokenize the train and test data\n",
        "tokenized_train = train_dataset.map(tokenize_function, batched = True)\n",
        "tokenized_test = test_dataset.map(tokenize_function, batched = True)\n",
        "\n",
        "# Define a data collator to handle padding\n",
        "from transformers import DataCollatorWithPadding\n",
        "data_collator = DataCollatorWithPadding(tokenizer = tokenizer)"
      ],
      "metadata": {
        "id": "XOygmXHFIOCF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the trainer and training arguments\n",
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "# Define the output directory and other training arguments\n",
        "output_dir_name = \"snli-hyp-distilbert\"\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "   output_dir = output_dir_name,\n",
        "   learning_rate = 2e-5,\n",
        "   per_device_train_batch_size = 16,\n",
        "   per_device_eval_batch_size = 16,\n",
        "   num_train_epochs = 1,\n",
        "   max_steps = 5000,\n",
        "   weight_decay = 0.01,\n",
        "   save_strategy = \"steps\",\n",
        "   save_steps = 500,\n",
        "   push_to_hub = False,\n",
        ")\n",
        "\n",
        "# Initialize the trainer\n",
        "trainer = Trainer(\n",
        "   model = model,\n",
        "   args = training_args,\n",
        "   train_dataset = tokenized_train,\n",
        "   tokenizer = tokenizer,\n",
        "   data_collator = data_collator,\n",
        ")"
      ],
      "metadata": {
        "id": "vQcmZaRsJj4P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "C0qUHJeC0h6q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ETS: <1min on colab T4 gpu\n",
        "all_preds, all_labels, acc, f1 = evaluate_model_nli(model, tokenizer, test_loader)"
      ],
      "metadata": {
        "id": "QuM-1JVa9mkm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihq84X2nSe2Q"
      },
      "source": [
        "As you see, the model is able to correctly guess the labels of almost **70%** of the NLI hypotheses without seeing what the premise is.\n",
        "\n",
        "Question: What do you think are ways that biases can be mitigated? Think about both the data collection process and model training for places where one can intervene."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6NTm7cJ8dqb"
      },
      "source": [
        "<a name=\"prompt0\"></a>\n",
        "## 2. Prompting\n",
        "\n",
        "The following sections will he based off the papers [Exploiting Cloze Questions for Few Shot Text Classification and Natural\n",
        "Language Inference](https://arxiv.org/pdf/2001.07676.pdf) and [How Many Data Points is a Prompt Worth?](https://arxiv.org/pdf/2103.08493.pdf).\n",
        "\n",
        "The first paper introduced Pattern Exploiting Training (PET), in which a NLP task is reformulated to a cloze style task for few shot learning. We will go into this a little more during the few-shot section of this lab. What you need to know for now, is that instead of training a model with a classification head, these models have a LM head to perform a classification task. Unlike language modeling, instead of predicting a word from the whole vocaulary, we are predicting a word from a list of verbalizers, where a word in the vocab is mapped to each label.\n",
        "\n",
        "\n",
        "We will be looking at classification tasks (NLI and sentiment) as they need only using one mask/single word verbalizers, however this paradigm can be extended to other tasks, with multiword verbalizers.\n",
        "\n",
        "First lets try **zero-shots prompting**. We will use `Roberta-large` for this section and investigate an easier `sentiment-analysis` task on IMDB dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EtDDLw-ryYEu"
      },
      "outputs": [],
      "source": [
        "test_dataset = load_dataset('imdb', split='test')\n",
        "\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-large')\n",
        "model = RobertaForMaskedLM.from_pretrained('roberta-large')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`TODO-2`: Complete the `lm_guess_sent` function to get the probability of each verbalizer in the `<mask>`, then make the prediction."
      ],
      "metadata": {
        "id": "deV4pRt2noeF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CildcznAK3h6"
      },
      "outputs": [],
      "source": [
        "def get_targets(verbalizer = 1):  #retreives the token ids for the verbalizers\n",
        "  targets = verbalize(verbalizer).keys()\n",
        "  target_ids = []\n",
        "  for target in targets:\n",
        "    id= tokenizer.get_vocab().get(\"\\u0120\"+ target, None) #how roberta ecodes wods\n",
        "    target_ids.append(id)\n",
        "  return target_ids\n",
        "\n",
        "def lm_guess_sent(model, text, template_num = 1, verb_num = 1, context_samples = None, context_labels = None):\n",
        "  if torch.cuda.is_available():\n",
        "    device = 'cuda:0'\n",
        "  else:\n",
        "    device = 'cpu'\n",
        "  model = model.to(device)\n",
        "\n",
        "  verbalizer = verbalize(verb_num) # choose a pair of verbalizers\n",
        "  target_ids = get_targets(verb_num) # get ids of verbalizers\n",
        "  text_template = template(text, template_num, context_samples=context_samples, context_labels=context_labels) # get a template with text\n",
        "\n",
        "  # TODO: encode texts with the template (text_template), return tensor\n",
        "  encoded_input = ...\n",
        "  encoded_input = encoded_input.to(device)\n",
        "\n",
        "  masked_index = torch.nonzero(encoded_input[\"input_ids\"][0] == tokenizer.mask_token_id, as_tuple=False).squeeze(-1).to(device) #getting index of mask token\n",
        "  model_outputs = model(**encoded_input)\n",
        "  outputs = model_outputs[\"logits\"]\n",
        "\n",
        "  # TODO: get the logits for masked tokens\n",
        "  logits = ...\n",
        "\n",
        "  probs = logits.softmax(dim=-1) # probability of tokens\n",
        "\n",
        "  # TODO: get the probability of the two verbalizer tokens\n",
        "  probs = ...\n",
        "\n",
        "  # TODO: get prediction as the index with higher probability\n",
        "  _, predictions = ...\n",
        "  input_ids = encoded_input[\"input_ids\"][0]\n",
        "  tokens = input_ids.detach().cpu().numpy().copy()\n",
        "  p = target_ids[predictions]\n",
        "\n",
        "  prediction = verbalizer[tokenizer.decode([p]).strip()] #get corresponging label from verbalizer\n",
        "  return prediction\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oO_e65cMAslb"
      },
      "source": [
        "<a name=\"prompt1\"></a>\n",
        "### 2.1 Zero-shot Prompting\n",
        "\n",
        "\n",
        "We will be using the IMDB dataset again to test prompting in the zero shot setting.\n",
        "\n",
        "We need two things to do the prompting\n",
        "\n",
        "- a **Verbalizer** that matches a word to each label\n",
        "- a **Template** to add the review, with one masked token that will predict one of the verbalizers\n",
        "\n",
        "Success of this method varies by template and verbalizer, so it is nice to test a few."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fekboRQ1d6xL"
      },
      "outputs": [],
      "source": [
        "def verbalize(num = 1):\n",
        "  if num == 1:\n",
        "    return {\"great\":1, \"horrible\":0}\n",
        "  if num == 2:\n",
        "    return {\"great\":1, \"terrible\":0}\n",
        "\n",
        "\n",
        "def template(text, num = 1, context_samples=None, context_labels=None):\n",
        "    if num == 1:\n",
        "      return \"It was <mask>.\" + text\n",
        "    if num == 2:\n",
        "      return \"So <mask>!\" + text\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ewv9hKEejyZP"
      },
      "source": [
        "Alright, lets see how the pre-trained roberta does on the prompted sentiment analysis.\n",
        "\n",
        "#### Verbalizer #1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WQUxhDj6Ojqo"
      },
      "outputs": [],
      "source": [
        "test_data_subset = pd.DataFrame(test_dataset[random.choices(range(len(test_dataset)), k=500)])\n",
        "\n",
        "guess = test_data_subset.apply(lambda x: lm_guess_sent(model, x['text'], template_num = 1, verb_num = 1), axis=1).tolist()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EDyc3IpDkIGR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1998b806-48a2-46b7-efcc-5cdccd97758d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy : 0.878\n",
            "Positive Accuracy : 0.95\n",
            "Negative Accuracy : 0.8115384615384615\n",
            "F1 : 0.878\n"
          ]
        }
      ],
      "source": [
        "test_data_subset['guess'] = guess\n",
        "\n",
        "print(\"Accuracy :\", accuracy_score(test_data_subset['label'], test_data_subset['guess']))\n",
        "print(\"Positive Accuracy :\", accuracy_score(test_data_subset[test_data_subset['label']==1]['label'], test_data_subset[test_data_subset['label']==1]['guess']))\n",
        "print(\"Negative Accuracy :\", accuracy_score(test_data_subset[test_data_subset['label']==0]['label'], test_data_subset[test_data_subset['label']==0]['guess']))\n",
        "print(\"F1 :\", f1_score(test_data_subset['label'], test_data_subset['guess'], average = 'micro'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2PyYK7LzDRE"
      },
      "source": [
        "It seems the first verbalizer works better for the Positive reviews. How can we improve the performance without retrain or finetune the model?\n",
        "\n",
        "#### Verbalizer #2\n",
        "Lets try different verbalizers (selection 2).\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zf8yGe65y8uU"
      },
      "outputs": [],
      "source": [
        "guess = test_data_subset.apply(lambda x: lm_guess_sent(model, x['text'], template_num = 1, verb_num = 2), axis=1).tolist()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yWubfN7Cy87R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "289304c9-152f-4b55-83a9-b7adb2d97752"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy : 0.898\n",
            "Positive Accuracy : 0.9125\n",
            "Negative Accuracy : 0.8846153846153846\n",
            "F1 : 0.898\n"
          ]
        }
      ],
      "source": [
        "test_data_subset['guess2'] = guess\n",
        "\n",
        "print(\"Accuracy :\", accuracy_score(test_data_subset['label'], test_data_subset['guess2']))\n",
        "print(\"Positive Accuracy :\", accuracy_score(test_data_subset[test_data_subset['label']==1]['label'], test_data_subset[test_data_subset['label']==1]['guess2']))\n",
        "print(\"Negative Accuracy :\", accuracy_score(test_data_subset[test_data_subset['label']==0]['label'], test_data_subset[test_data_subset['label']==0]['guess2']))\n",
        "\n",
        "print(\"F1 :\", f1_score(test_data_subset['label'], test_data_subset['guess2'], average = 'micro'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Thinking**: The second verbalizer seems work well with ~90% accuracy in both classes! Why do you think this formulation of the task works in the zero-shot setting? Can you think of any ways to *pick the most effective verbalizers* in a more systematic way?\n",
        "\n",
        "You can feel free to try your own templates/verbalizers to see how your design choices affect performance, and which ones could improve performance."
      ],
      "metadata": {
        "id": "Hgj5mdSZvknh"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbsGhu95ux4s"
      },
      "source": [
        "<a name=\"prompt2\"></a>\n",
        "### 2.2 Few-shot Prompting\n",
        "\n",
        "Now given that we have an access to a very small labeled dataset (e.g. 5 samples), how can we make a great use of these information?\n",
        "\n",
        "If we finetune the model on these 5 samples, the model is very likely to overfit to some biased shortcuts. **Recall the prompting trick, do we have some ways to re-design the template to combine the labeled samples?**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = load_dataset('imdb', split='train')\n",
        "train_data = train_data.shuffle(seed=42)\n",
        "fewshot_samples = train_data.select(range(10))\n",
        "\n",
        "context_samples = fewshot_samples['text']\n",
        "context_labels = fewshot_samples['label']"
      ],
      "metadata": {
        "id": "pY3UnDeMI1D9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N9Nh96XoW-t8"
      },
      "outputs": [],
      "source": [
        "def verbalize(num = 1):\n",
        "  if num == 1:\n",
        "    return {\"great\":1, \"horrible\":0}\n",
        "  if num == 2:\n",
        "    return {\"great\":1, \"terrible\":0}\n",
        "\n",
        "\n",
        "def template(text, num = 1, context_samples = None, context_labels = None):\n",
        "    if num == 1:\n",
        "      temp = \"It was <mask>.\" + text\n",
        "      pos_prefix = \"It was great.\"\n",
        "      neg_prefix = \"It was horrible.\"\n",
        "    elif num == 2:\n",
        "      temp = \"So <mask>!\" + text\n",
        "      pos_prefix = \"It was great.\"\n",
        "      neg_prefix = \"It was terrible.\"\n",
        "    else:\n",
        "      raise NotImplemented\n",
        "\n",
        "    # Build 'Context' with few-shot labeled samples\n",
        "    if context_samples is not None:\n",
        "      assert context_labels is not None, 'Please provide labels to the few-shot samples!'\n",
        "      context = ''\n",
        "      for c,y in zip(context_samples, context_labels):\n",
        "        if y==0:\n",
        "          context += (neg_prefix+' '.join(c.split(' ')[:25])+'//')\n",
        "        elif y==1:\n",
        "          context += (pos_prefix+' '.join(c.split(' ')[:25])+'//')\n",
        "      return context+temp\n",
        "    return temp\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's see how the template would look like\n",
        "template(test_data_subset['text'][0], num = 1, context_samples = context_samples, context_labels = context_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "VpqnmIEzI8a7",
        "outputId": "597ac396-98e7-4197-979b-8ff4c06272f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'It was great.There is no relation at all between Fortier and Profiler but the fact that both are police series about violent crimes. Profiler looks crispy, Fortier//It was great.This movie is a great. The plot is very true to the book which is a classic written by Mark Twain. The movie starts of//It was horrible.George P. Cosmatos\\' \"Rambo: First Blood Part II\" is pure wish-fulfillment. The United States clearly didn\\'t win the war in Vietnam. They caused damage to//It was great.In the process of trying to establish the audiences\\' empathy with Jake Roedel (Tobey Maguire) the filmmakers slander the North and the Jayhawkers. Missouri never//It was horrible.Yeh, I know -- you\\'re quivering with excitement. Well, *The Secret Lives of Dentists* will not upset your expectations: it\\'s solidly made but essentially unimaginative,//It was great.While this movie\\'s style isn\\'t as understated and realistic as a sound version probably would have been, this is still a very good film. In//It was great.I give this movie 7 out of 10 because the villains were interesting in their roles and the unknown batwoman creates an interesting \"guess who\"//It was horrible.really awful... lead actor did OK... the film, plot etc was completely crap and inaccurate it may as well have been a sequel to well...//It was horrible.Good grief I can\\'t even begin to describe how poor this film is. Don\\'t get me wrong, I wasn\\'t expecting much to begin with. Let\\'s//It was great.Home Room deals with a Columbine-like high-school shooting but rather than hashing over the occurrence itself the film portrays the aftermath and what happened to//It was <mask>.OK, this simply is the worst movie ever made. Period. Horrible acting, sets and music. Ok, everything sucks in this movie. I almost forgot! The special effects are \"great\" also. So if you like bad movies, watch this, it can surely make you laugh!!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bc8rGlX4wbdX"
      },
      "source": [
        "Then we can test how the model performs with **few-shot prompting**.\n",
        "\n",
        "#### Verbalizer number 1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "guess = test_data_subset.apply(lambda x: lm_guess_sent(model, x['text'], template_num = 1, verb_num = 1, context_samples=context_samples, context_labels=context_labels), axis=1).tolist()\n"
      ],
      "metadata": {
        "id": "U1DzL_36KkDG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data_subset['10shots-guess'] = guess\n",
        "\n",
        "print(\"Accuracy :\", accuracy_score(test_data_subset['label'], test_data_subset['10shots-guess']))\n",
        "print(\"Positive Accuracy :\", accuracy_score(test_data_subset[test_data_subset['label']==1]['label'], test_data_subset[test_data_subset['label']==1]['10shots-guess']))\n",
        "print(\"Negative Accuracy :\", accuracy_score(test_data_subset[test_data_subset['label']==0]['label'], test_data_subset[test_data_subset['label']==0]['10shots-guess']))\n",
        "print(\"F1 :\", f1_score(test_data_subset['label'], test_data_subset['10shots-guess'], average = 'micro'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3MtrsFIehPko",
        "outputId": "2b2e15bc-722b-43c4-eeca-43a033d91422"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy : 0.556\n",
            "Positive Accuracy : 1.0\n",
            "Negative Accuracy : 0.059322033898305086\n",
            "F1 : 0.556\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGeQANEOY7Bu"
      },
      "source": [
        "#### Verbalizer number 2\n",
        "\n",
        "Now we will use different verbalizers to see how the model performs."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "guess = test_data_subset.apply(lambda x: lm_guess_sent(model, x['text'], template_num = 1, verb_num = 2, context_samples=context_samples, context_labels=context_labels), axis=1).tolist()\n"
      ],
      "metadata": {
        "id": "ULH7ghzWjZ-k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data_subset['10shots-guess2'] = guess\n",
        "\n",
        "print(\"Accuracy :\", accuracy_score(test_data_subset['label'], test_data_subset['10shots-guess2']))\n",
        "print(\"Positive Accuracy :\", accuracy_score(test_data_subset[test_data_subset['label']==1]['label'], test_data_subset[test_data_subset['label']==1]['10shots-guess2']))\n",
        "print(\"Negative Accuracy :\", accuracy_score(test_data_subset[test_data_subset['label']==0]['label'], test_data_subset[test_data_subset['label']==0]['10shots-guess2']))\n",
        "print(\"F1 :\", f1_score(test_data_subset['label'], test_data_subset['10shots-guess2'], average = 'micro'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MaTn6mhAjaYd",
        "outputId": "7c245345-5a25-40d7-9dcd-019879e29bda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy : 0.528\n",
            "Positive Accuracy : 1.0\n",
            "Negative Accuracy : 0.0\n",
            "F1 : 0.528\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Thinking**: What do you think of the performance? Why do you think it could happen? What can we do to improve?\n",
        "\n",
        "\n",
        "Feel free to process or change the prompts/contexts as you like, then you can see how your design choices could influence the few-shot prompting performance :)"
      ],
      "metadata": {
        "id": "EN5VRZ2hlCO-"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.2"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}